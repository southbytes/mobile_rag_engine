// local-gemma-macos/lib/services/ollama_response_service.dart
//
// Service for generating LLM responses with RAG context
// Handles response mode selection (STRICT/HYBRID/FALLBACK) and prompt construction

import 'package:flutter/foundation.dart';
import 'package:mobile_rag_engine/mobile_rag_engine.dart';
import 'package:ollama_dart/ollama_dart.dart';

/// Response mode based on RAG similarity scores
enum ResponseMode {
  strict, // High similarity (>= 0.7): use only document context
  hybrid, // Medium similarity (>= 0.5): combine document + general knowledge
  fallback, // Low/no similarity: use general knowledge
}

/// Result of response generation
class OllamaResponseResult {
  final String response;
  final ResponseMode mode;
  final double bestSimilarity;

  const OllamaResponseResult({
    required this.response,
    required this.mode,
    required this.bestSimilarity,
  });
}

/// Service for generating Ollama LLM responses with RAG context
class OllamaResponseService {
  final OllamaClient ollamaClient;
  final String modelName;

  // Thresholds for response mode selection
  static const double hybridThreshold = 0.5;
  static const double strictThreshold = 0.7;

  OllamaResponseService({
    required this.ollamaClient,
    this.modelName = 'gemma3:4b',
  });

  /// Generate a response using RAG context
  /// Returns the response text and metadata
  Future<OllamaResponseResult> generateResponse({
    required String query,
    required String contextText,
    required RagSearchResult ragResult,
    required bool hasRelevantContext,
    required List<Message> chatHistory,
    void Function(Message)? onHistoryUpdate,
  }) async {
    // Calculate best similarity score for mode decision
    final bestSimilarity = _calculateBestSimilarity(ragResult);

    // Determine response mode
    final mode = _determineResponseMode(hasRelevantContext, bestSimilarity);

    debugPrint(
      'ğŸ¯ Response Mode: ${mode.name.toUpperCase()} '
      '(bestSim: ${bestSimilarity.toStringAsFixed(3)})',
    );

    try {
      // Build messages
      final messages = <Message>[];

      // 1. System Prompt - varies by mode
      messages.add(_buildSystemPrompt(mode));

      // 2. Chat History (last 6 messages)
      final historyStart = chatHistory.length > 6 ? chatHistory.length - 6 : 0;
      messages.addAll(chatHistory.sublist(historyStart));

      // 3. Current User Message (WITH RAG CONTEXT)
      final userMessage = _buildUserMessage(query, contextText, mode);
      messages.add(Message(role: MessageRole.user, content: userMessage));

      // Save raw query to history (not the huge context prompt)
      onHistoryUpdate?.call(Message(role: MessageRole.user, content: query));

      // Debug: Log prompt structure
      debugPrint('ğŸ“¨ === Prompt to LLM ===');
      debugPrint('ğŸ“¨ System: ${messages[0].content}');
      debugPrint('ğŸ“¨ History: ${chatHistory.length} messages');
      debugPrint('ğŸ“¨ User Query: $query');
      debugPrint('ğŸ“¨ Context Length: ${contextText.length} chars');
      debugPrint('ğŸ“¨ Mode: ${mode.name}');

      // Stream response from Ollama
      final responseBuffer = StringBuffer();
      final thinkingBuffer = StringBuffer();
      bool isInThinking = false;
      int chunkCount = 0;

      debugPrint('ğŸ“ === LLM Streaming Start ===');

      final stream = ollamaClient.generateChatCompletionStream(
        request: GenerateChatCompletionRequest(
          model: modelName,
          messages: messages,
        ),
      );

      await for (final chunk in stream) {
        final content = chunk.message.content;
        responseBuffer.write(content);
        chunkCount++;

        // Detect thinking/reasoning sections (some models use <think> tags)
        if (content.contains('<think>')) {
          isInThinking = true;
          debugPrint('ğŸ§  [THINKING START]');
        }
        if (content.contains('</think>')) {
          isInThinking = false;
          debugPrint('ğŸ§  [THINKING END]');
        }

        // Log chunk content
        if (isInThinking) {
          thinkingBuffer.write(content);
          // Print thinking chunks with special prefix
          final cleanContent = content.replaceAll('\n', 'â†µ');
          debugPrint('ğŸ§  $cleanContent');
        } else {
          // Print response chunks
          final cleanContent = content.replaceAll('\n', 'â†µ');
          if (cleanContent.isNotEmpty) {
            // debugPrint('ğŸ’¬ $cleanContent');
          }
        }
      }

      debugPrint('ğŸ“ === LLM Streaming End ($chunkCount chunks) ===');

      // Log thinking summary if any
      if (thinkingBuffer.isNotEmpty) {
        debugPrint('ğŸ§  === Thinking Summary ===');
        debugPrint(thinkingBuffer.toString());
        debugPrint('ğŸ§  === End Thinking ===');
      }

      final response = responseBuffer.toString().trim();

      // Save assistant response to history
      onHistoryUpdate?.call(
        Message(role: MessageRole.assistant, content: response),
      );

      if (response.isEmpty) {
        return OllamaResponseResult(
          response:
              'âš ï¸ The model returned an empty response. Please try again.',
          mode: mode,
          bestSimilarity: bestSimilarity,
        );
      }

      return OllamaResponseResult(
        response: response,
        mode: mode,
        bestSimilarity: bestSimilarity,
      );
    } catch (e, stackTrace) {
      debugPrint('ğŸ”´ Ollama Error: $e');
      debugPrint('ğŸ”´ Stack Trace: $stackTrace');

      return OllamaResponseResult(
        response:
            'âš ï¸ Ollama Error: $e\n\n'
            'Make sure Ollama is running (ollama serve) and the model is installed.',
        mode: mode,
        bestSimilarity: bestSimilarity,
      );
    }
  }

  /// Calculate best similarity score from RAG results
  double _calculateBestSimilarity(RagSearchResult ragResult) {
    if (ragResult.chunks.isEmpty) return 0.0;

    return ragResult.chunks
        .map((c) => c.similarity)
        .where((s) => s > 0) // Exclude adjacent chunks with 0.0
        .fold(0.0, (a, b) => a > b ? a : b);
  }

  /// Determine response mode based on context and similarity
  ResponseMode _determineResponseMode(
    bool hasRelevantContext,
    double bestSimilarity,
  ) {
    if (!hasRelevantContext) return ResponseMode.fallback;
    if (bestSimilarity >= strictThreshold) return ResponseMode.strict;
    if (bestSimilarity >= hybridThreshold) return ResponseMode.hybrid;
    return ResponseMode.fallback;
  }

  /// Build system prompt based on response mode
  Message _buildSystemPrompt(ResponseMode mode) {
    switch (mode) {
      case ResponseMode.strict:
        return const Message(
          role: MessageRole.system,
          content:
              'ë‹¹ì‹ ì€ ì œê³µëœ ë¬¸ë§¥ì„ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ëŠ” AI ë¹„ì„œì…ë‹ˆë‹¤. '
              'ë¬¸ë§¥ì— ìˆëŠ” ì •ë³´ë¥¼ ìš°ì„ í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.',
        );
      case ResponseMode.hybrid:
        return const Message(
          role: MessageRole.system,
          content:
              'ë‹¹ì‹ ì€ ì œê³µëœ ë¬¸ë§¥ê³¼ ì¼ë°˜ ì§€ì‹ì„ ê²°í•©í•˜ì—¬ ë‹µë³€í•˜ëŠ” AI ë¹„ì„œì…ë‹ˆë‹¤. '
              'ë¬¸ë§¥ì˜ ì •ë³´ë¥¼ ìš°ì„ í•˜ë˜, í•„ìš”ì‹œ ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë³´ì™„í•˜ì„¸ìš”. '
              'ë‹¨, ë¬¸ë§¥ì—ì„œ ì˜¨ ì •ë³´ì™€ ì¼ë°˜ ì§€ì‹ì„ êµ¬ë¶„í•˜ì—¬ ì„¤ëª…í•˜ì„¸ìš”.',
        );
      case ResponseMode.fallback:
        return const Message(
          role: MessageRole.system,
          content: 'ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ë¹„ì„œì…ë‹ˆë‹¤.',
        );
    }
  }

  /// Build user message with context based on response mode
  String _buildUserMessage(
    String query,
    String contextText,
    ResponseMode mode,
  ) {
    switch (mode) {
      case ResponseMode.strict:
        return '''
[ì°¸ê³  ë¬¸ì„œ]
$contextText
[ì°¸ê³  ë¬¸ì„œ ì¢…ë£Œ]

ìœ„ ë¬¸ì„œì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

ì§ˆë¬¸: $query''';

      case ResponseMode.hybrid:
        return '''
[ê´€ë ¨ ë¬¸ì„œ]
$contextText
[ê´€ë ¨ ë¬¸ì„œ ì¢…ë£Œ]

ìœ„ ë¬¸ì„œì— ê´€ë ¨ ë‚´ìš©ì´ ìˆìŠµë‹ˆë‹¤. ë¬¸ì„œ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•˜ë˜, 
í•„ìš”í•œ ê²½ìš° ì¼ë°˜ì ì¸ ì§€ì‹ìœ¼ë¡œ ë³´ì™„í•´ë„ ë©ë‹ˆë‹¤.
ë¬¸ì„œì—ì„œ ì§ì ‘ í™•ì¸ëœ ë‚´ìš©ê³¼ ì¼ë°˜ ì§€ì‹ì„ êµ¬ë¶„í•´ì„œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.

ì§ˆë¬¸: $query''';

      case ResponseMode.fallback:
        return '''
ì§ˆë¬¸: $query

ì°¸ê³ : ì—…ë¡œë“œëœ ë¬¸ì„œì—ì„œ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.
ì¼ë°˜ì ì¸ ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•˜ë˜, ë” ì •í™•í•œ ì •ë³´ê°€ í•„ìš”í•˜ë©´ ê´€ë ¨ ë¬¸ì„œë¥¼ ì¶”ê°€í•´ë‹¬ë¼ê³  ì•ˆë‚´í•˜ì„¸ìš”.''';
    }
  }
}
